{"project": "pyg", "project_url": "https://pyg.org", "show_commit_url": "https://github.com/pyg-team/pytorch_geometric/commit/", "hash_length": 8, "revision_to_hash": {"1084": "9eeb18d61feb56cc98cd81e46af2270d768d53f1", "1152": "8313ae159dcd50e49690e60badae2330c32cfb2b", "1209": "964530024bbeb52a9bb014a60f29b499e646cad9", "1341": "c99e0ea85064779219084f68b515150c7f3bb8a4", "1478": "7b3f16b4bbf9bf9560a4fd25095fbab6810ac790", "1489": "253bd1457110ec0030aa7f6ede30f2fcec9fc494", "1571": "636dbc48d264cc3e75eda3f5d7947e815ffbd907", "1719": "6de2a1bc9c56060ef5afb6e4ba06c8cce86a2269", "1730": "e4589bac6e710b18496f1e0b750c5ed4446fe50a", "1750": "751a411ee9d7f396ca070c19f948a72d78e97089", "1888": "d5aff37604c8e3247f5e807f2ba0ec6eeb4c661b", "1985": "0b2a01c2bfd255b17f15657578ab9bf143212973", "2294": "5c2399d06cfb921f3168ced69a1406746a2a2447", "2515": "58ff9037e4c3dd4653aa37a387c03a37a8f0d546", "2551": "881d5ba2aefc26328eeeaa17fd7ef6daaae06ef4", "2707": "a83d38dbd0e51599139fb86af2371aef4c3ad4ca", "2727": "5bb5db9e105e6531ac4da22f2674f5614e2d1732", "2846": "9ad551ff68dd1172d140973598c90d696aa7e152", "3102": "cc071b7c4bd632ace8919a81d7049b984e09f0ba", "3608": "31dca649cca1c329efa98ef608953538ddff7b74", "3687": "df0612bca76b14464b13c86759eec0c56ebc8154", "3941": "c57eb7d60d672462ac3eaa6476c2f5fe78dc9f3c", "3955": "4b531e5168555c5c2600cc616a21aa050306efb4", "4361": "e6b8d6427ad930c6117298006d7eebea0a37ceac", "4777": "d86de00a98173653a6158fc40238d34d0fb57cc1", "4827": "5455b528d58e7d5b57f0046313065c160c4d4caa", "5009": "c5152eae0de1bfaec1b60a7f931e0c928bb6df3c", "5023": "82a3bab57e4d5b8695e546e56536bee37545642e", "5144": "101ed2cb5cc15344f41dc2f062b5284d8327dece", "5309": "d47d9cde477e1ab25821178968538eddeabc351a", "5518": "97d55577f1d0bf33c1bfbe0ef864923ad5cb844d", "5856": "07bf02f6bd871d3e1571a995ec53da461c11a11c", "6316": "ca4e5f8e308cf4ee7a221cb0979bb12d9e37a318", "7307": "dd0610a95935a4d8b00974464251190356084118", "7342": "152ab41e3f6211a780b19b7a5850c1b60e21767e", "7392": "76337403fc3e67163c8c60618cda1f6e61b1fd15", "7447": "b2c846f32ed1028cc6fec8c984031e9019d126a2", "7494": "ff9fb3d7cdbaa2f2b8a848fb6cc5f4c3f465118c", "7530": "d7daf0a74ad685a8d51efaac1706a7a7f3cae6c6", "7663": "3c671c4614abb8a830985e6c25d3243a6ee60e27", "7707": "38da3c68f0be67feb6cc77584ee68bcd32059739", "7758": "d974f519be26d0356d538237ffb7c36fd9a73e3d", "7814": "d2da4dacddff0015d9fb7459b0df53b6907f194d", "7844": "ba54c72512859bdd4996f5e64ea5566d5d74db0a", "7915": "085f2cc64dc18cecc88878b9ae3fa0ec7f7e5739"}, "revision_to_date": {"1084": 1527237156000, "1152": 1534165740000, "1209": 1540007891000, "1341": 1545136515000, "1478": 1547578922000, "1489": 1548423127000, "1571": 1551978830000, "1719": 1554100892000, "1730": 1554182914000, "1750": 1554441998000, "1888": 1556522735000, "1985": 1558511934000, "2294": 1561817893000, "2515": 1567064345000, "2551": 1570176475000, "2707": 1580817773000, "2727": 1582039081000, "2846": 1584467720000, "3102": 1590382943000, "3608": 1594103054000, "3687": 1596633598000, "3941": 1606460050000, "3955": 1606920803000, "4361": 1617954430000, "4777": 1623915629000, "4827": 1624695909000, "5009": 1631516432000, "5023": 1631774083000, "5144": 1635246629000, "5309": 1640155409000, "5518": 1647084760000, "5856": 1660732188000, "6316": 1669877069000, "7307": 1679559864000, "7342": 1680061402000, "7392": 1681081762000, "7447": 1681915051000, "7494": 1680252540000, "7530": 1683461695000, "7663": 1684349060000, "7707": 1685081879000, "7758": 1685867013000, "7814": 1686576173000, "7844": 1687360323000, "7915": 1688629386000}, "params": {"machine": ["codespace-v100"], "python": ["3.10"], "branch": ["master"]}, "graph_param_list": [{"machine": "codespace-v100", "python": "3.10", "branch": "master"}], "benchmarks": {"import.Import.timeraw_import": {"code": "class Import:\n    def timeraw_import(self):\n        return \"import torch_geometric\"", "min_run_count": 2, "name": "import.Import.timeraw_import", "number": 1, "param_names": [], "params": [], "repeat": 0, "rounds": 1, "sample_time": 0.01, "timeout": 60.0, "type": "time", "unit": "seconds", "version": "c2b5fc8c6e18b51e356434a3c7e2fdf7b2acf11a46a24fbf26e84365ab1dc22f", "warmup_time": -1}, "utils.Map.track_exclusive": {"code": "class Map:\n    def track_exclusive(self, *_):\n        t = Timer(\n            stmt=\"f(src, index[:50_000], None, False)\",\n            globals=self.globals,\n            num_threads=4,\n            label=\"map\",\n            sub_label=\" \",\n            description=\" \",\n        )\n        m = t.blocked_autorange(min_run_time=1)\n        return m.median * 1_000**2  # us\n\n    def setup(self, *params):\n        f, device = params\n        src = torch.randint(0, 100_000_000, (100_000,), device=device)\n        index = src.unique()\n        self.globals = {\n            \"f\": f,\n            \"src\": src,\n            \"index\": index,\n        }", "name": "utils.Map.track_exclusive", "param_names": ["f", "device"], "params": [["<function trivial_map>", "<function map_index>"], ["'cpu'"]], "timeout": 60.0, "type": "track", "unit": "us", "version": "e2720ec233129047b6a9c1d8c1bd2d4d95f66c0251401973c19971e8957080a7"}, "utils.Map.track_inclusive": {"code": "class Map:\n    def track_inclusive(self, *_):\n        t = Timer(\n            stmt=\"f(src, index, None, True)\",\n            globals=self.globals,\n            num_threads=4,\n            label=\"map\",\n            sub_label=\" \",\n            description=\" \",\n        )\n        m = t.blocked_autorange(min_run_time=1)\n        return m.median * 1_000**2  # us\n\n    def setup(self, *params):\n        f, device = params\n        src = torch.randint(0, 100_000_000, (100_000,), device=device)\n        index = src.unique()\n        self.globals = {\n            \"f\": f,\n            \"src\": src,\n            \"index\": index,\n        }", "name": "utils.Map.track_inclusive", "param_names": ["f", "device"], "params": [["<function trivial_map>", "<function map_index>"], ["'cpu'"]], "timeout": 60.0, "type": "track", "unit": "us", "version": "154ed80729002314b21755096f756a90dd92c2f5125b75d8dff13c032a73fb1a"}, "utils.Scatter.track_bwd": {"code": "class Scatter:\n    def track_bwd(self, *params):\n        f, *_ = params\n        t = Timer(\n            stmt=\"out.backward(out_grad, retain_graph=True)\",\n            setup=(\n                f\"from {__name__} import {f.__name__}, grads_like\\n\"\n                f\"out = {f.__name__}(x, index, dim_size, reduce)\\n\"\n                f\"out_grad = grads_like(out)\"\n            ),\n            globals=self.globals,\n            num_threads=4,\n            label=\"scatter\",\n            sub_label=f.__name__,\n            description=self.globals[\"reduce\"],\n        )\n        m = t.blocked_autorange(min_run_time=1)\n        return m.median * 1_000**2  # us\n\n    def setup(self, *params):\n        f, reduce, (num_nodes, num_edges), device = params\n    \n        if f is own_scatter and not WITH_TORCH_SCATTER:\n            raise NotImplementedError(\"torch-scatter not found\", WITH_TORCH_SCATTER)\n    \n        if f is pytorch_index_add and reduce != \"sum\":\n            raise NotImplementedError\n    \n        self.globals = {\n            \"x\": torch.randn(num_edges, 64, device=device, requires_grad=True),\n            \"index\": torch.randint(num_nodes, (num_edges,), device=device),\n            \"dim_size\": num_nodes,\n            \"reduce\": reduce,\n        }", "name": "utils.Scatter.track_bwd", "param_names": ["f", "reduce", "num_nodes, num_edges", "device"], "params": [["<function pytorch_scatter>", "<function own_scatter>", "<function optimized_scatter>", "<function pytorch_index_add>"], ["'sum'", "'mean'", "'min'", "'max'", "'mul'"], ["(4000, 200000)", "(16000, 800000)", "(64000, 3200000)"], ["'cuda'"]], "timeout": 60.0, "type": "track", "unit": "us", "version": "3b227c3164b1589dc91210463b41cf682a75087179d6e5522da467abe1598a89"}, "utils.Scatter.track_fwd": {"code": "class Scatter:\n    def track_fwd(self, *params):\n        f, *_ = params\n        t = Timer(\n            stmt=f\"{f.__name__}(x, index, dim_size, reduce)\",\n            setup=f\"from {__name__} import {f.__name__}\",\n            globals=self.globals,\n            num_threads=4,\n            label=\"scatter\",\n            sub_label=f.__name__,\n            description=self.globals[\"reduce\"],\n        )\n        m = t.blocked_autorange(min_run_time=1)\n        return m.median * 1_000**2  # us\n\n    def setup(self, *params):\n        f, reduce, (num_nodes, num_edges), device = params\n    \n        if f is own_scatter and not WITH_TORCH_SCATTER:\n            raise NotImplementedError(\"torch-scatter not found\", WITH_TORCH_SCATTER)\n    \n        if f is pytorch_index_add and reduce != \"sum\":\n            raise NotImplementedError\n    \n        self.globals = {\n            \"x\": torch.randn(num_edges, 64, device=device, requires_grad=True),\n            \"index\": torch.randint(num_nodes, (num_edges,), device=device),\n            \"dim_size\": num_nodes,\n            \"reduce\": reduce,\n        }", "name": "utils.Scatter.track_fwd", "param_names": ["f", "reduce", "num_nodes, num_edges", "device"], "params": [["<function pytorch_scatter>", "<function own_scatter>", "<function optimized_scatter>", "<function pytorch_index_add>"], ["'sum'", "'mean'", "'min'", "'max'", "'mul'"], ["(4000, 200000)", "(16000, 800000)", "(64000, 3200000)"], ["'cuda'"]], "timeout": 60.0, "type": "track", "unit": "us", "version": "001a4aba95fdbd19e806bfa561c53ac5b5396519e078068effa2c869b713d510"}, "utils.Softmax.track_bwd": {"code": "class Softmax:\n    def track_bwd(self, *_):\n        t = Timer(\n            stmt=\"out.backward(out_grad, retain_graph=True)\",\n            setup=f\"from {__name__} import grads_like; out = f(x, index); out_grad = grads_like(out)\",\n            globals=self.globals,\n            num_threads=1,\n            label=\"softmax_bwd\",\n            sub_label=\" \",\n            description=\" \",\n        )\n        m = t.blocked_autorange(min_run_time=1)\n        return m.median * 1_000**2  # us\n\n    def setup(self, *params):\n        f, compile, (num_nodes, num_edges), device = params\n        self.globals = {\n            \"f\": torch_geometric.compile(f) if compile else f,\n            \"x\": torch.randn(num_edges, 64, device=device),\n            \"index\": torch.randint(num_nodes, (num_edges,), device=device),\n        }", "name": "utils.Softmax.track_bwd", "param_names": ["f", "compile", "num_nodes, num_edges", "device"], "params": [["<function softmax>", "<function dense_softmax>"], ["False", "True"], ["(10000, 200000)"], ["'cuda'"]], "timeout": 60.0, "type": "track", "unit": "us", "version": "87a5e3ac0a0d8c75a6ce50461318817c1e11e37e23f7840edffc515aa849ebf6"}, "utils.Softmax.track_fwd": {"code": "class Softmax:\n    def track_fwd(self, *_):\n        t = Timer(\n            stmt=\"f(x, index)\",\n            globals=self.globals.copy(),\n            num_threads=4,\n            label=\"softmax_fwd\",\n            sub_label=\" \",\n            description=\" \",\n        )\n        m = t.blocked_autorange(min_run_time=1)\n        return m.median * 1_000**2  # us\n\n    def setup(self, *params):\n        f, compile, (num_nodes, num_edges), device = params\n        self.globals = {\n            \"f\": torch_geometric.compile(f) if compile else f,\n            \"x\": torch.randn(num_edges, 64, device=device),\n            \"index\": torch.randint(num_nodes, (num_edges,), device=device),\n        }", "name": "utils.Softmax.track_fwd", "param_names": ["f", "compile", "num_nodes, num_edges", "device"], "params": [["<function softmax>", "<function dense_softmax>"], ["False", "True"], ["(10000, 200000)"], ["'cuda'"]], "timeout": 60.0, "type": "track", "unit": "us", "version": "59dd090377b1301b280752080045b57d92ccebb6fa16502d8c37ac37769f3f62"}, "utils.Sparse.track_fwd": {"code": "class Sparse:\n    def track_fwd(self, *params):\n        f, *_ = params\n        t = Timer(\n            stmt=\"f(edge_index, None, (size, size))\",\n            globals=self.globals,\n            num_threads=4,\n            label=\"sparse\",\n            sub_label=f.__name__,\n            description=\" \",\n        )\n        m = t.blocked_autorange(min_run_time=1)\n        return m.median * 1_000**2  # us\n\n    def setup(self, *params):\n        f, (num_nodes, num_edges), device = params\n    \n        self.globals = {\n            \"f\": f,\n            \"edge_index\": torch.randint(num_nodes, (2, num_edges), device=device),\n            \"size\": num_nodes,\n        }", "name": "utils.Sparse.track_fwd", "param_names": ["f", "num_nodes, num_edges", "device"], "params": [["<bound method SparseTensor.from_edge_index of <class 'torch_geometric.typing.SparseTensor'>>", "<function to_torch_coo_tensor>", "<function to_torch_csr_tensor>", "<function to_torch_csc_tensor>"], ["(10000, 200000)"], ["'cuda'"]], "timeout": 60.0, "type": "track", "unit": "us", "version": "d7f4b3a84c6b7ecc064d8c6a7f111fa670e3f8ba016fc0841497f2386a90f3cf"}, "utils.Spmm.track_bwd": {"code": "class Spmm:\n    def track_bwd(self, *params):\n        layout, *_ = params\n        t = Timer(\n            stmt=\"out.backward(out_grad, retain_graph=True)\",\n            setup=f\"from torch_geometric.utils import spmm; from {__name__} import grads_like; out = spmm(adj, x, reduce); out_grad = grads_like(out)\",\n            globals=self.globals,\n            num_threads=4,\n            label=\"spmm_bwd\",\n            sub_label=layout,\n            description=\" \",\n        )\n        m = t.blocked_autorange(min_run_time=1)\n        return m.median * 1_000**2  # us\n\n    def setup(self, *params):\n        layout, reduce, (num_nodes, num_edges), device = params\n        x = torch.randn(num_nodes, 64, device=device, requires_grad=True)\n        edge_index = torch.randint(num_nodes, (2, num_edges), device=device)\n        adj = to_torch_coo_tensor(edge_index, size=num_nodes).to_sparse(layout=layout)\n        self.globals = {\n            \"adj\": adj,\n            \"x\": x,\n            \"reduce\": reduce,\n        }", "name": "utils.Spmm.track_bwd", "param_names": ["layout", "reduce", "num_nodes, num_edges", "device"], "params": [["torch.sparse_coo", "torch.sparse_csr", "torch.sparse_csc"], ["'sum'", "'mean'"], ["(10000, 200000)"], ["'cuda'"]], "timeout": 60.0, "type": "track", "unit": "us", "version": "18665e458f6276be48de5355345e775724efb5748d9d30df122ed8d4a5a31a57"}, "utils.Spmm.track_fwd": {"code": "class Spmm:\n    def track_fwd(self, *params):\n        layout, *_ = params\n        t = Timer(\n            stmt=\"spmm(adj, x, reduce)\",\n            setup=f\"from torch_geometric.utils import spmm\",\n            globals=self.globals,\n            num_threads=4,\n            label=\"spmm\",\n            sub_label=layout,\n            description=\" \",\n        )\n        m = t.blocked_autorange(min_run_time=1)\n        return m.median * 1_000**2  # us\n\n    def setup(self, *params):\n        layout, reduce, (num_nodes, num_edges), device = params\n        x = torch.randn(num_nodes, 64, device=device, requires_grad=True)\n        edge_index = torch.randint(num_nodes, (2, num_edges), device=device)\n        adj = to_torch_coo_tensor(edge_index, size=num_nodes).to_sparse(layout=layout)\n        self.globals = {\n            \"adj\": adj,\n            \"x\": x,\n            \"reduce\": reduce,\n        }", "name": "utils.Spmm.track_fwd", "param_names": ["layout", "reduce", "num_nodes, num_edges", "device"], "params": [["torch.sparse_coo", "torch.sparse_csr", "torch.sparse_csc"], ["'sum'", "'mean'"], ["(10000, 200000)"], ["'cuda'"]], "timeout": 60.0, "type": "track", "unit": "us", "version": "ee636b097b54bf6d33b0e36f1f16357035a2f29f63a5b0f22a40fc4691959fd7"}}, "machines": {"codespace-v100": {"machine": "codespace-v100", "version": 1}}, "tags": {"0.1.1": 1084, "0.3.0": 1152, "0.3.1": 1209, "1.0.0": 1341, "1.0.1": 1478, "1.0.2": 1489, "1.0.3": 1571, "1.1.0": 1719, "1.1.1": 1730, "1.1.2": 1750, "1.2.0": 1888, "1.2.1": 1985, "1.3.0": 2294, "1.3.1": 2515, "1.3.2": 2551, "1.4.1": 2707, "1.4.2": 2727, "1.4.3": 2846, "1.5.0": 3102, "1.6.0": 3608, "1.6.1": 3687, "1.6.2": 3941, "1.6.3": 3955, "1.7.0": 4361, "1.7.1": 4777, "1.7.2": 4827, "2.0.0": 5009, "2.0.1": 5023, "2.0.2": 5144, "2.0.3": 5309, "2.0.4": 5518, "2.1.0": 5856, "2.2.0": 6316, "2.3.0": 7307, "2.3.1": 7494}, "pages": [["", "Grid view", "Display as a agrid"], ["summarylist", "List view", "Display as a list"], ["regressions", "Show regressions", "Display information about recent regressions"]]}